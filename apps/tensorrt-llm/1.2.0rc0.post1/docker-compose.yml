services:
  vllm:
    image: nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc0.post1
    container_name: ${CONTAINER_NAME}
    restart: always
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - 1panel-network
    volumes:
      - ${MODEL_PATH}:/models
    ports:
      - ${PANEL_APP_PORT_HTTP}:8000
    ipc: host
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65535
        hard: 65535
      stack: 67108864
    command: bach -c "trtllm-serve /models/${MODEL_NAME} --host 0.0.0.0 --port 8000"
networks:
  1panel-network:
    external: true
